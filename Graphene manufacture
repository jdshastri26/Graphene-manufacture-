


import torch
import torch.nn as nn
import torch.optim as optim

# Example synthetic dataset
# Features might represent different conditions in graphene manufacturing
# such as temperature, pressure, duration, etc.
features = torch.tensor([
    [0.5, 0.2, 0.1],
    [0.9, 0.5, 0.6],
    [0.4, 0.1, 0.2],
    [0.7, 0.8, 0.9],
    [0.1, 0.4, 0.5]
], dtype=torch.float32)

# Labels representing electrical conductivity
labels = torch.tensor([
    [0.8],
    [0.9],
    [0.7],
    [0.95],
    [0.65]
], dtype=torch.float32)

# Define a simple neural network model
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(3, 10)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(10, 1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

# Initialize the model, loss function, and optimizer
model = SimpleNN()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    
    # Forward pass
    outputs = model(features)
    loss = criterion(outputs, labels)
    
    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Test the model (in practice, you'd use a separate test set)
model.eval()
with torch.no_grad():
    predicted = model(features)
    print(f'Predicted Conductivity: {predicted.numpy()}')
    print(f'Actual Conductivity: {labels.numpy()}')



Explanation:

    Data Preparation: We create a synthetic dataset with input features (e.g., manufacturing conditions) and corresponding labels (e.g., electrical conductivity).

    Model Definition: A simple neural network with one hidden layer.

    Training Loop: The model is trained for a specified number of epochs using the Mean Squared Error (MSE) loss function and the Adam optimizer.

    Evaluation: After training, we evaluate the model's predictions on the training data.


